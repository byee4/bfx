{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to overlap CLIP signal with conservation phastcon scores.\n",
    "- careful of the regions chosen for noncoding_exon and noncoding_intron.\n",
    "- Improvement over first pass which did not consider premature boundaries, this notebook is careful that each plotted region is completely within a CDS/UTR/Intron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bay001/anaconda2/envs/brian/lib/python2.7/site-packages/matplotlib/__init__.py:1350: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "sys.path.insert(0, '/home/bay001/projects/codebase/rbp-maps/maps/')\n",
    "from density import Map\n",
    "from density import ReadDensity\n",
    "from density import normalization_functions\n",
    "from density import RDPlotter\n",
    "import glob\n",
    "import functools\n",
    "import pybedtools\n",
    "from qtools import Submitter\n",
    "colors = sns.color_palette('hls',14)\n",
    "center_dir = '/home/bay001/projects/encode/analysis/conservation_analysis/idr_peak_centers2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create single point bedfiles for each peak\n",
    "- each peak 'center' will be used to define the peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dfbd58af0c4cf1b6b0efa296ee91f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "181/|/100%|| 181/181 [00:30<00:00,  9.11it/s]"
     ]
    }
   ],
   "source": [
    "suffix_in = 'merged.bed'\n",
    "suffix_out = 'merged.center.bed'\n",
    "\n",
    "def get_midpoint_and_create_bed(fin, fout):\n",
    "    \"\"\"\n",
    "    From a BED file, create a new BED file containing the start/stop as the midpoint/midpoint+1\n",
    "    \"\"\"\n",
    "    peak = pd.read_table(\n",
    "        fin, sep='\\t', names=['chrom','start','stop','name','score','strand']\n",
    "    )\n",
    "    peak['mid'] = ((peak['start'] + peak['stop'])/2).astype(int)\n",
    "    peak['mid1'] = peak['mid'] + 1\n",
    "    peak[['chrom','mid','mid1','name','score','strand']].to_csv(fout, sep='\\t', header=False, index=False)\n",
    "\n",
    "def run_cell_wrapper(center_dir):\n",
    "    \"\"\"\n",
    "    Since we really only do this once, wrap this cell into a function and run when needed.\n",
    "    \"\"\"\n",
    "    all_beds = glob.glob(os.path.join(center_dir, \"*{}\".format(suffix_in)))\n",
    "    progress = tnrange(len(all_beds))\n",
    "    for bed in all_beds:\n",
    "        fout = bed.replace(suffix_in, suffix_out)\n",
    "        get_midpoint_and_create_bed(bed, fout)\n",
    "        progress.update(1)\n",
    "        \n",
    "run_cell_wrapper(center_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate all center beds\n",
    "- Runs the annotation script to generate annotations for each peak center.\n",
    "- Need to annotate each center peak since no guarantee that overlapped broadpeaks will be annotated the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing 181 tasks as an array-job.\n",
      "Wrote commands to /home/bay001/projects/encode/analysis/conservation_analysis/bash_scripts/annotate_peakcenters.sh.\n",
      "Submitted script to queue home-yeo.\n",
      " Job ID: 8935733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<qtools.submitter.Submitter at 0x2b7d69cbab50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_in = 'merged.center.bed'\n",
    "suffix_out = 'merged.center.bed.annotated'\n",
    "\n",
    "center_beds = glob.glob(os.path.join(center_dir, \"*{}\".format(suffix_in)))\n",
    "cmds = []\n",
    "for bed in center_beds:\n",
    "    annotated = bed.replace(suffix_in, suffix_out)\n",
    "    if not os.path.exists(annotated):\n",
    "        cmd = 'python /home/bay001/projects/codebase/annotator/annotate.py '\n",
    "        cmd = cmd + '--gtfdb /projects/ps-yeolab/genomes/hg19/gencode_v19/gencode.v19.annotation.gtf.db '\n",
    "        cmd = cmd + '--input {} '.format(bed)\n",
    "        cmd = cmd + '--output {}'.format(annotated)\n",
    "        cmds.append(cmd)\n",
    "job_name = 'annotate_peakcenters'\n",
    "sh = '/home/bay001/projects/encode/analysis/conservation_analysis/bash_scripts/annotate_peakcenters.sh'\n",
    "\n",
    "Submitter(\n",
    "    commands=cmds, job_name=job_name, sh=sh, array=True, submit=True, queue='home-yeo', ppn=4,\n",
    "    walltime='4:00:00'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse center beds to create a twobed file\n",
    "- Use the annotated transcript region as upper/lower boundaries for each peak center.\n",
    "- 'twobed' will be a file with 12 columns: the first 6 columns will describe everything 'less than position wise' the peak center, the second 6 will describe anything 'greater than position wise' the peak center. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c2dea23b0a417ea37fa56b819291a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suffix_in = 'merged.center.bed.annotated'\n",
    "suffix_out = 'merged.center.bed.annotated.twobed'\n",
    "### 'merged.center.bed.annotated.CDS.twobed'\n",
    "### 'merged.center.bed.annotated.3UTR.twobed'\n",
    "### 'merged.center.bed.annotated.5UTR.twobed'\n",
    "### 'merged.center.bed.annotated.intron.twobed'\n",
    "### 'merged.center.bed.annotated.noncoding_intron.twobed'\n",
    "### 'merged.center.bed.annotated.noncoding_exon.twobed'\n",
    "\n",
    "def parse_feature(row, col):\n",
    "    \"\"\"\n",
    "    Parses each annotated row and returns the 'col' value \n",
    "    Or returns INTERGENIC if the row has no other annotation.\n",
    "    \n",
    "    col : int\n",
    "    \n",
    "    \"\"\"\n",
    "    if 'INTERGENIC' not in row['priority']:\n",
    "        return row['priority'].split(':')[col]\n",
    "    return 'INTERGENIC'\n",
    "\n",
    "def annotated_to_twobed(annotated, twobed_out):\n",
    "    \"\"\"\n",
    "    Writes a 'twobed' file using the annotated 'center peak' file created before.\n",
    "    \"\"\"\n",
    "    df = pd.read_table(\n",
    "        annotated,\n",
    "        names=['chrom','start','stop','p','f','strand','priority','annotation']\n",
    "    )\n",
    "    df['region'] = df.apply(functools.partial(parse_feature, col=4), axis=1)\n",
    "    df['start_boundary'] = df.apply(functools.partial(parse_feature, col=1), axis=1)\n",
    "    df['stop_boundary'] = df.apply(functools.partial(parse_feature, col=2), axis=1)\n",
    "    twobed = df[['chrom','start_boundary','stop','region','f','strand','chrom','stop','stop_boundary','region','f','strand']]\n",
    "    twobed.to_csv(twobed_out, sep='\\t', header=False, index=False)\n",
    "    for region in set(df['region']):\n",
    "        df_r = df[df['region']==region]\n",
    "        twobed_r = df_r[['chrom','start_boundary','stop','region','f','strand',\n",
    "                         'chrom','stop','stop_boundary','region','f','strand'\n",
    "                        ]]\n",
    "        twobed_r.to_csv(twobed_out.replace('.twobed','.{}.twobed'.format(region)), sep='\\t', index=False, header=False)\n",
    "\n",
    "# for all annotated center files, create a twobed file.\n",
    "\n",
    "all_annotated = glob.glob(os.path.join(center_dir, '*{}'.format(suffix_in)))\n",
    "progress = tnrange(len(all_annotated))\n",
    "\n",
    "for annotated in all_annotated:\n",
    "    twobed = annotated.replace(suffix_in, suffix_out)\n",
    "    annotated_to_twobed(annotated, twobed)\n",
    "    progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate 'full peaks' bedfiles using the annotations from the 'center' bedfiles\n",
    "- re-join the annotated 'center' beds to the 'full peak' beds. we need to do this because the 'peak center annotated' bed files will not always match the 'peak annotated' bed files (due to possible overlapping features)\n",
    "- This creates a fully annotated list of full peaks for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8758ad81564493a136da70feace182"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_peak_suffix_in = 'merged.bed'\n",
    "center_peak_suffix_in = 'merged.center.bed.annotated'\n",
    "\n",
    "# THESE FILES ARE CREATED BUT NOT EXPLICITLY NAMED AS full_peak_suffix_out\n",
    "\n",
    "### full_peak_suffix_out = 'merged.bed.annotated.CDS.bed'\n",
    "### full_peak_suffix_out = 'merged.bed.annotated.3UTR.bed'\n",
    "### full_peak_suffix_out = 'merged.bed.annotated.5UTR.bed'\n",
    "### full_peak_suffix_out = 'merged.bed.annotated.intron.bed'\n",
    "### full_peak_suffix_out = 'merged.bed.annotated.noncoding_intron.bed'\n",
    "### full_peak_suffix_out = 'merged.bed.annotated.noncoding_exon.bed'\n",
    "\n",
    "genome = '/projects/ps-yeolab/genomes/hg19/hg19.chrom.sizes'\n",
    "\n",
    "regions_bedfiles = {\n",
    "    'CDS':'/home/bay001/projects/encode/analysis/conservation_analysis/region_bedfiles/hg19_v19_cds.bed',\n",
    "    '3UTR':'/home/bay001/projects/encode/analysis/conservation_analysis/region_bedfiles/hg19_v19_three_prime_utrs.bed',\n",
    "    '5UTR':'/home/bay001/projects/encode/analysis/conservation_analysis/region_bedfiles/hg19_v19_five_prime_utrs.bed',\n",
    "    'intron':'/home/bay001/projects/encode/analysis/conservation_analysis/region_bedfiles/hg19_v19_introns.bed',\n",
    "    'noncoding_intron':'/home/bay001/projects/encode/analysis/conservation_analysis/region_bedfiles/hg19_v19_introns.bed',\n",
    "    'noncoding_exon':'/home/bay001/projects/encode/analysis/conservation_analysis/region_bedfiles/hg19_v19_exons.bed'\n",
    "}\n",
    "\n",
    "\n",
    "# dff = 'full peak' dataframe\n",
    "# dfc = 'center peak' dataframe\n",
    "# dfm = 'merged full + center' annotated dataframe\n",
    "# dfr = 'merged full + center' annotated dataframe split into regions\n",
    "\n",
    "\n",
    "def merge_and_parse_center_peaks(peak_center_bed_annotated, peak_full_bed):\n",
    "    \"\"\"\n",
    "    merges the 'center-annotated' peaks with the full unannotated peaks to return\n",
    "    the full peak list annotated.\n",
    "    \"\"\"\n",
    "    dff = pd.read_table(peak_full_bed, names=['chrom','start','stop','name','score','strand'])\n",
    "    dfc = pd.read_table(peak_center_bed_annotated, names=['chrom','c_start','c_stop','name','score','strand', 'priority', 'annotation'])\n",
    "    dfc['region'] = dfc.apply(functools.partial(parse_feature, col=4), axis=1)\n",
    "    dff['c_start'] = ((dff['start'] + dff['stop'])/2).astype(int)\n",
    "    dff['c_stop'] = dff['c_start'] + 1\n",
    "    dfm = pd.merge(dfc, dff, how='left', on=['chrom','c_start','c_stop','strand'])\n",
    "    dfm[['chrom','start','stop','region','score_x','strand']]\n",
    "    for region in set(dfm['region']):\n",
    "        dfr = dfm[dfm['region']==region]\n",
    "        dfr = dfr[['chrom','start','stop','priority','score_x','strand']]\n",
    "        dfr.to_csv(peak_full_bed.replace('merged.bed','merged.bed.annotated.{}.bed'.format(region)), sep='\\t', header=False, index=False)\n",
    "\n",
    "all_peaks = glob.glob(os.path.join(center_dir, '*{}'.format(full_peak_suffix_in)))\n",
    "progress = tnrange(len(all_peaks))\n",
    "for peak in all_peaks:\n",
    "    peak_center = peak.replace(full_peak_suffix_in, center_peak_suffix_in)\n",
    "    if os.path.exists(peak) and os.path.exists(peak_center):\n",
    "        merge_and_parse_center_peaks(\n",
    "            peak_center, peak\n",
    "        )\n",
    "    progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random bedfiles from these 'full peak' bedfiles\n",
    "- use the annotations to decide which region to bin each peak into.\n",
    "- this produces the same number of random peak files for each region (region is defined by the 'center peak' locations, random peak length is defined by the 'full peak' lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc4cbe5aef44360a99420add54566a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a6dc46f32c44348de7dffeb1976820"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f3e76a40834177b98545d415b2e1c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50df76e337e040839cb21e42819d999b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing 500 tasks as an array-job.\n",
      "Wrote commands to /home/bay001/projects/encode/analysis/conservation_analysis/bash_scripts/random_shuffle1.sh.\n",
      "Submitted script to queue home-yeo.\n",
      " Job ID: 8936152\n",
      "Writing 209 tasks as an array-job.\n",
      "Wrote commands to /home/bay001/projects/encode/analysis/conservation_analysis/bash_scripts/random_shuffle2.sh.\n",
      "Submitted script to queue home-yeo.\n",
      " Job ID: 8936153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<qtools.submitter.Submitter at 0x2b7d69fedcd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "181/|/100%|| 181/181 [00:15<00:00, 559.02it/s]"
     ]
    }
   ],
   "source": [
    "### suffix_in = '*merged.bed.annotated.{}.bed'.format(region)\n",
    "### suffix_out = '*merged.bed.annotated.{}.RAND.bed'.format(region)\n",
    "\n",
    "regions = ['CDS','3UTR','5UTR','intron']\n",
    "cmds = []\n",
    "for region in regions:\n",
    "    suffix_in = '*merged.bed.annotated.{}.bed'.format(region)\n",
    "    region_peaks = glob.glob(os.path.join(center_dir, suffix_in))\n",
    "    region_bed = regions_bedfiles[region]\n",
    "    progress = tnrange(len(region_peaks), desc=region, leave=False)\n",
    "    \n",
    "    for peak_file in region_peaks:\n",
    "        if not os.path.exists(\n",
    "            peak_file.replace(\".{}.bed\".format(region), \".{}.RAND.bed\".format(region))\n",
    "        ):\n",
    "            cmd = 'bedtools shuffle '\n",
    "            cmd = cmd + '-i {} '.format(peak_file)\n",
    "            cmd = cmd + '-g {} '.format(genome)\n",
    "            cmd = cmd + '-chrom '\n",
    "            cmd = cmd + '-incl {} '.format(region_bed)\n",
    "            cmd = cmd + '> {}'.format(peak_file.replace(\".{}.bed\".format(region), \".{}.RAND.bed\".format(region)))\n",
    "            cmds.append(cmd)\n",
    "        progress.update(1)\n",
    "        \n",
    "job_name = 'random_shuffle'\n",
    "sh = '/home/bay001/projects/encode/analysis/conservation_analysis/bash_scripts/random_shuffle.sh'\n",
    "\n",
    "Submitter(\n",
    "    commands=cmds, \n",
    "    job_name=job_name, \n",
    "    sh=sh, \n",
    "    array=True, submit=True, \n",
    "    queue='home-yeo', ppn=1,\n",
    "    walltime='12:00:00'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now re-annotate these regions\n",
    "- we need to do this to get the new boundaries for each region (cannot use the older boundaries as these are new random locations)\n",
    "- we CANNOT annotate the 'random peak centers' since due to peaks spanning multiple regions, we should not expect all 'centerized' points to fall in the original region.\n",
    "- use a special priority list that gives first priority to whatever region we expect. So for a '3UTR' list of random-center peaks first priority needs to be given to the 3'UTR\n",
    "- i want to make this unstranded, because sometimes the annotator will not find anything because the region exists on the opposite strand... causing some random peaks to be called as 'intergenic'. Right now, I don't care about strandedness (this is random right?), and only care about the fact that all of these peaks are annotated as 'intronic/CDS/UTR/etc.' and that I can get the proper boundaries for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing 461 tasks as an array-job.\n",
      "Wrote commands to /home/bay001/projects/encode/analysis/conservation_analysis/bash_scripts/annotate_rand_peaks.sh.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<qtools.submitter.Submitter at 0x2b7d6a387690>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_in = 'RAND.bed'\n",
    "suffix_out = 'RAND.bed.annotated'\n",
    "\n",
    "regions = ['CDS','3UTR','5UTR','intron']\n",
    "priority_dir = '/home/bay001/projects/encode/analysis/conservation_analysis/priorty_lists/' # this is jsut the same priority list, except we push each respective region to the top.\n",
    "cmds = []\n",
    "for region in regions:\n",
    "    priority = os.path.join(priority_dir, '{}priority.txt'.format(region))\n",
    "\n",
    "    center_beds = glob.glob(os.path.join(center_dir, \"*{}.{}\".format(region, suffix_in)))\n",
    "    \n",
    "    for bed in center_beds:\n",
    "        annotated = bed.replace(suffix_in, suffix_out)\n",
    "        if not os.path.exists(annotated):\n",
    "            cmd = 'python /home/bay001/projects/codebase/annotator/annotate.py '\n",
    "            cmd = cmd + '--gtfdb /projects/ps-yeolab/genomes/hg19/gencode_v19/gencode.v19.annotation.gtf.db '\n",
    "            cmd = cmd + '--input {} '.format(bed)\n",
    "            cmd = cmd + '--output {} '.format(annotated)\n",
    "            cmd = cmd + '--transcript-priority-file {} '.format(priority)\n",
    "            cmd = cmd + '--gene-priority-file {} '.format(priority)\n",
    "            cmd = cmd + '--unstranded'\n",
    "            cmds.append(cmd)\n",
    "job_name = 'annotate_rand_peaks'\n",
    "sh = '/home/bay001/projects/encode/analysis/conservation_analysis/bash_scripts/annotate_rand_peaks.sh'\n",
    "\n",
    "Submitter(\n",
    "    commands=cmds, job_name=job_name, sh=sh, array=True, submit=False, queue='home-yeo', ppn=8,\n",
    "    walltime='4:00:00'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the 'center-bed' of these RAND regions\n",
    "- basically make the 'center.RAND' bed files\n",
    "- since the full peaks are guaranteed to be completely contained within each region, the 'centers' should also be completely contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suffix_in = 'RAND.bed.annotated'\n",
    "suffix_out = 'RAND.center.bed.annotated'\n",
    "\n",
    "def get_midpoint_and_create_bed(fin, fout):\n",
    "    \"\"\"\n",
    "    From a BED file, create a new BED file containing the start/stop as the midpoint/midpoint+1\n",
    "    \"\"\"\n",
    "    peak = pd.read_table(\n",
    "        fin, sep='\\t', names=['chrom','start','stop','name','score','strand','priority','annotation']\n",
    "    )\n",
    "    peak['mid'] = ((peak['start'] + peak['stop'])/2).astype(int)\n",
    "    peak['mid1'] = peak['mid'] + 1\n",
    "    peak[['chrom','mid','mid1','priority','score','strand']].to_csv(fout, sep='\\t', header=False, index=False)\n",
    "\n",
    "    \n",
    "def run_wrapper(center_dir):\n",
    "    \"\"\"\n",
    "    Since we really only do this once, wrap this cell into a function and run when needed.\n",
    "    \"\"\"\n",
    "    all_beds = glob.glob(os.path.join(center_dir, \"*{}\".format(suffix_in)))\n",
    "    print(\"found {} beds\".format(len(all_beds)))\n",
    "    progress = tnrange(len(all_beds))\n",
    "    for bed in all_beds:\n",
    "        fout = bed.replace(suffix_in, suffix_out)\n",
    "        get_midpoint_and_create_bed(bed, fout)\n",
    "        progress.update(1)\n",
    "run_wrapper(center_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then create twobed files from these center bed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suffix_in = '.RAND.center.bed.annotated'\n",
    "suffix_out = '.RAND.center.bed.annotated.twobed'\n",
    "\n",
    "def parse_feature(row, col):\n",
    "    \"\"\"\n",
    "    This is identical to the 'parse_feature' function before, \n",
    "    but now we look at the 'name' instead of 'priority'\n",
    "    \n",
    "    Parses each annotated row and returns the 'col' value \n",
    "    Or returns INTERGENIC if the row has no other annotation.\n",
    "    \n",
    "    my annotation string looks like: \n",
    "        transcript_id:txstart:txend:strand:region:gene_id:gene_name:coding/noncoding:overlap\n",
    "        \n",
    "    col : int\n",
    "    \n",
    "    \"\"\"\n",
    "    if 'INTERGENIC' not in row['name']:\n",
    "        return row['name'].split(':')[col]\n",
    "    return 'INTERGENIC'\n",
    "\n",
    "def truncate_on_boundaries(row):\n",
    "    \"\"\"\n",
    "    takes four positions: start-boundary, start, stop, stop-boundary\n",
    "    and uses the 'start/stop' boundaries as hard cutoffs for a given region.\n",
    "    If either start/stop fall outside of the boundary regions, return the midpoint of the \n",
    "    boundaries, guaranteeing that we stay within. \n",
    "    \"\"\"\n",
    "    if row['start'] <= int(row['start_boundary']) or row['stop'] >= int(row['stop_boundary']):\n",
    "        return int((int(row['start_boundary']) + int(row['stop_boundary']))/2)\n",
    "    else:\n",
    "        return row['start']\n",
    "\n",
    "def annotated_rand_to_twobed(annotated, twobed_out):\n",
    "    \"\"\"\n",
    "    Writes a 'twobed' file using the annotated 'center peak' file created before.\n",
    "    \n",
    "    * bedtools shuffle created non-completely-overlapping regions that only \n",
    "    partially cover the -incl CDS/intron/UTR regions and are NOT strand specific\n",
    "    (ie. \n",
    "        [chr16   22111599        22111697] was created using the bedtools command:\n",
    "        \n",
    "        wd=/home/bay001/projects/encode/analysis/conservation_analysis/idr_peak_centers\n",
    "        \n",
    "        bedtools shuffle -i \\\n",
    "        ${wd}/idr_peak_centers/693.01v02.IDR.out.0102merged.bed.annotated.CDS.bed -g \\\n",
    "        /projects/ps-yeolab/genomes/hg19/hg19.chrom.sizes \\\n",
    "        -chrom \\\n",
    "        -incl ${wd}/region_bedfiles/hg19_v19_cds.bed > \\\n",
    "        ${wd}/idr_peak_centers/693.01v02.IDR.out.0102merged.bed.annotated.CDS.RAND.bed\n",
    "        \n",
    "        where chr16:22111599-22111697 is not completely contained within any of \n",
    "        the cds.bed intervals.\n",
    "    )\n",
    "    \n",
    "    *** THEREFORE let's just hack the region to return just the upper/lower boundaries of the real region.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_table(\n",
    "            annotated,\n",
    "            names=['chrom','start','stop','name','f','strand']\n",
    "        )\n",
    "\n",
    "        df['start_boundary'] = df.apply(functools.partial(parse_feature, col=1), axis=1)\n",
    "        df['stop_boundary'] = df.apply(functools.partial(parse_feature, col=2), axis=1)\n",
    "        df['region'] = df.apply(functools.partial(parse_feature, col=4), axis=1)\n",
    "        \n",
    "        twobed = df[[\n",
    "            'chrom','start_boundary','start','region','f','strand',\n",
    "            'chrom','start','stop_boundary','region','f','strand'\n",
    "        ]]\n",
    "        twobed.columns = [ # rename just the start -> stop column\n",
    "            'chrom','start_boundary','start','region','f','strand',\n",
    "            'chrom','stop','stop_boundary','region','f','strand'\n",
    "        ]\n",
    "        # assert twobed['start_boundary'] < twobed['stop_boundary']  # these MUST define a proper transcript region.\n",
    "        twobed['start'] = twobed.apply(truncate_on_boundaries, axis=1)\n",
    "        twobed['stop'] = twobed.apply(truncate_on_boundaries, axis=1)\n",
    "        # assert twobed['start'] == twobed['stop']\n",
    "        \n",
    "        twobed.to_csv(twobed_out, sep='\\t', index=False, header=False)\n",
    "    except ValueError as e:\n",
    "        print(annotated, e)\n",
    "# for all annotated center files, create a twobed file.\n",
    "\n",
    "all_annotated = glob.glob(os.path.join(center_dir, '*{}'.format(suffix_in)))\n",
    "progress = tnrange(len(all_annotated))\n",
    "\n",
    "for annotated in all_annotated:\n",
    "    twobed = annotated.replace(suffix_in, suffix_out)\n",
    "    annotated_rand_to_twobed(annotated, twobed)\n",
    "    progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate matrices for each map\n",
    "- now we have: 1) a 'centerized' bedfile annotated about the feature that contains it, and 2) a random 'centerized' bedfile annotated about its feature that contains it. Both files should contain the same number of peaks, whose original peak sizes should also be the same.\n",
    "- do 50nt in, 50 nt out from center peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phastcon = '/projects/ps-yeolab/genomes/hg19/hg19.100way.phastCons.bw'\n",
    "\n",
    "def create_matrix(twobed, phastcon):\n",
    "    annotation = {twobed:'twobeds'}\n",
    "    output_filename = '/home/bay001/projects/encode/analysis/tests/tmp/test.txt'\n",
    "    phast = ReadDensity.Phastcon(phastcon)\n",
    "    m = Map.PhastconMap(\n",
    "        phastcon=phast,\n",
    "        annotation=annotation,\n",
    "        output_filename=output_filename,\n",
    "        upstream_offset=50,downstream_offset=50,\n",
    "        min_density_threshold=0,\n",
    "    )\n",
    "\n",
    "    m.create_matrices()\n",
    "    return normalization_functions.clean(m.raw_matrices['phastcon'][twobed])    \n",
    "\n",
    "def plot_matrix_and_mean(real, random, output_file):\n",
    "    \"\"\"\n",
    "    Takes two 'twobed' files, creates the matrices of phastcon scores, and plots the mean\n",
    "    \"\"\"\n",
    "    real_matrix = create_matrix(real, phastcon)\n",
    "    random_matrix = create_matrix(random, phastcon)\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20,20))\n",
    "    if real_matrix.shape[0] != random_matrix.shape[0]:  # we should have the same number of 'peaks' \n",
    "        print(\"warning: event nums don't match: {}, {}\".format(\n",
    "            real, random\n",
    "        ))\n",
    "    elif real_matrix.shape[0] < 100:\n",
    "        return 0\n",
    "    else:\n",
    "        real_matrix.to_csv(output_file.replace('.png','.real.txt'), sep='\\t')\n",
    "        random_matrix.to_csv(output_file.replace('.png','.random.txt'), sep='\\t')\n",
    "        \n",
    "        ax1.plot(real_matrix.mean(), label=os.path.basename(real).replace('.twobed',''), color='blue')\n",
    "        ax1.plot(random_matrix.mean(), label=os.path.basename(random).replace('.twobed',''), color='red')\n",
    "\n",
    "        sns.heatmap(real_matrix, yticklabels=False, xticklabels=False, ax=ax2)\n",
    "        ax2.set_title('real matrix heatmap')\n",
    "        sns.heatmap(random_matrix, yticklabels=False, xticklabels=False, ax=ax3)\n",
    "        ax3.set_title('rand matrix heatmap')\n",
    "        ax1.legend(loc=1)\n",
    "        f.savefig(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir = '/home/bay001/projects/encode/analysis/conservation_analysis/conservation_plots/7-3-17'\n",
    "err = 0  # number of times we didn't find both real and random paired center twobed files\n",
    "\n",
    "for region in ['CDS','3UTR','5UTR','intron']:\n",
    "    all_region_beds = glob.glob(os.path.join(center_dir, \"*.{}.*twobed\".format(region)))\n",
    "    all_region_uids = list(set([os.path.basename(bed).split('.')[0] for bed in all_region_beds]))\n",
    "    progress = tnrange(len(all_region_uids), desc=region, leave=True)\n",
    "    for uid in all_region_uids:\n",
    "        real_and_rand_pair = glob.glob(os.path.join(center_dir, \"{}*.{}.*twobed\".format(uid, region)))\n",
    "        if len(real_and_rand_pair) == 2: # found both real and random center twobed files\n",
    "            # some kind of ordering step.\n",
    "            random = ''\n",
    "            real = ''\n",
    "            for p in real_and_rand_pair:\n",
    "                if 'RAND.center.bed' in p:\n",
    "                    random = p\n",
    "                else:\n",
    "                    real = p\n",
    "            output_file = os.path.join(output_dir, os.path.basename(real).replace('.twobed','.png'))\n",
    "            # print(\"plotting: [{}], [{}]\".format(real, random))\n",
    "            if not os.path.exists(output_file):\n",
    "                plot_matrix_and_mean(real, random, output_file)\n",
    "        else:\n",
    "            err += 1\n",
    "        progress.update(1)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# warning: event nums don't match: /home/bay001/projects/encode/analysis/conservation_analysis/idr_peak_centers/350.01v02.IDR.out.0102merged.center.bed.annotated.intron.twobed, /home/bay001/projects/encode/analysis/conservation_analysis/idr_peak_centers/350.01v02.IDR.out.0102merged.bed.annotated.intron.RAND.center.bed.annotated.twobed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
